---
title: "State Level Modeling"
author: "Julie Wisch, Oliver Causey, Song Yuanhong, Charles Yeh, Gavan Tredoux"
date: "10/5/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 5, fig.height = 3, fig.align = "center")
library(data.table)
library(readxl)
library(ggplot2)
library(gridExtra)
library(grid)
library(tidyr)
library(parsnip)
library(Metrics)
library(gridExtra)
library(tidyverse)
library(usmap)
library(glmnet)
library(mgcv)

devtools::load_all("../.././BPI/")

```

## Background

The Bayer Crop Science (BCS) team successfully built a model that could generate national level predictions for acres planted with a 30 - 70% reduction in error as compared to the freely available USDA national level projections. The purpose of this document is to investigate modeling at the state level. We find that we can generate state-level predictions with a 40 - 80% reduction in error as compared to the naive case (assuming what was planted last year will be planted this year). The USDA does not release state-level projections, so we were unable to compare our model performance to a USDA forecast.

### USDA Planting Projections and Total Plantings

The USDA publishes many crop planting projections annually, including both corn and soy. These projections are released at a national level only. No state or regional level projections are published.

It is the BCS team's understanding that these models are developed in October, preliminarily released in November, and re-released in final form in February. 

The USDA also conducts annual surveys in the first two weeks of March in order to project the total number of acres planted in the US for that spring. The USDA contacts more than 100,000 farmers across the United States. These surveys are re-conducted in June and August of the same year in order to update the planting totals. The USDA uses these probabilistic surveys to calculate the total number of acres planted annually. These totals can be found here: https://quickstats.nass.usda.gov/results/823A5054-B8F9-3341-AC31-8B03C6990BC1 . These surveys are conducted at the FIPS-code level and aggregated to both the state and national level. Below is a sample of the available data, showing the reported planted acreage of corn by year for the state of Illinois.

```{r}
window <- 8 #size of sliding window for training
useRegularization <-T #apply lasso regularization to limit the number of features in the model?
nFeatures <- 30 #number of features to keep after regularization to be used in XGBoost model


source("../.././randomforest/StateLevelModeling_DataCleaning_RMD.R") #Cleaning file to generate state_df

tmp <- state_df[state_df$state_name == "ILLINOIS",]
tmp$Value <- tmp$Value / 10^6
ggplot(tmp, aes(x = year, y = Value)) + geom_line() + theme_bw() +
  xlab("Year") + ylab("Planted Acreage, Illinois (Mill. of Acres)")

```

\newpage

### Modeling Approach

The proposed BCS model is a simple linear regression using a sliding 8 year training window.
 $$
 \begin{align}
 State~Level~Planting~Acreage_{8~year~window} = \beta_{0} + \beta_{1}State
 \end{align}
 $$
A sliding 8 year window means  that we train on 8 years of data and test on the 9th. First we train the model on years 2000 - 2007, and then test on year 2008; then, we train the model on years 2001 - 2008 and test on year 2009, etc. We test on years 2008 - 2019, which means that mean average errors are calculated off of 12 datapoints. We would caution the reader against over-interpreting the quality of the model based on this limited number of testing points.

The team considered several other more complex models, including:

 * Lasso Regression
 * Generalized Additive Models
 * XGBoost Modeling, with and without lasso regression for feature regularization

The more complex models considered the following features:

 * Historical national planting corn acreage (source: USDA)
 * Historical state level planting corn acreage (source: USDA)
 * Historical state level planting soy acreage (source: USDA)
 * Historical price received per bushel of corn, state level (source: USDA)
 * State (this variable was target encoded using last year's state level planting acreage as the response)
 * Historical state level sales (source: Internal Data)
 * In season state level sales as they become available (source: Internal Data)
 * USDA Model Projection for National Planting (source: USDA)
 * Estimated USDA State Level Projection for Planting:
 
 $$
 \begin{align}
 USDA~Projection_{thisyear} \left( \frac{State~Level~Planting_{lastyear}}{National~Level~Planting_{lastyear}}
  \right)
 \end{align}
 $$
 
Despite including substantially more information, none of the models the team evaluated showed a significant lift beyond the estimate that the very basic linear regression could provide. 

```{r}
model_parameters <- c("acre_corn_actual_national_lag1", "acre_corn_actual_national_lag2",
                      "AcresPlanted_Soy_lag1", "AcresPlanted_Soy_lag2",
                      "OneYearPriorProjection_USDA",
                      "Value_lag1", "Value_lag2",
                      "TargetEncodedState",
                      names(state_df)[grepl('Sales', colnames(state_df)) & grepl('lag', colnames(state_df))],
                      "estimatedFutureAcreage",
                      "PriceReceivedDollarsperBushel_lag1", "PriceReceivedDollarsperBushel_lag2")


############### Additional Data Prep
#Getting Lagged Features
state_df <- get_laggedfeaturesbysplit(state_df, "state_name", c("Value", "AcresPlanted_Soy", "PriceReceivedDollarsperBushel",
                                                    names(state_df)[grepl('Sales', colnames(state_df))]), 2)

#Target encode the states
TargEncState <- read.csv(paste0("../.././data/processed/target_encoded_state_window", window))
TargEncState$testyear <- TargEncState$FinalYearofTraining + 1
state_df <- merge(state_df, TargEncState, by.x = c("year", "state_name"),
                  by.y = c("testyear", "state_name"),
                   all = FALSE) #dropping state dataframe down to 1999 - 2018, after using 1992 - 1997 to train target encoding only
rm(TargEncState)

#Creating an estimate of future acreage combining the % of acres planted by state last year with the USDA's future projection for planting
state_df$estimatedFutureAcreage <- (state_df$Value_lag1 / state_df$acre_corn_actual_national_lag1) *
                                    state_df$OneYearPriorProjection_USDA
```

## Results

The BCS team produced several models that were all better than the naive prediction (that what happened last year will happen this year). Below we show the results of the XGBoost model (performed following regularization), the GAM, and the simple linear model on the 10 largest states for corn planting.

```{r}
#Then start modeling
if(useRegularization == T){
 #Lasso for regularization
  features_to_keep <- get_reducedfeaturelist(state_df, window,
                                           model_parameters, nFeatures)
 }else{
  features_to_keep <- model_parameters
}

#########
xgb_mode <- boost_tree() %>%
  set_mode("regression") %>%
  set_engine("xgboost")

model_result <- get_modelpredictions(features_to_keep, xgb_mode)
totalresult_xgb_DF <- model_result[[1]]
feature_importance <- model_result[[2]]

MAE_forplot <- totalresult_xgb_DF[,c("state_name", "year", "AbsPercErr", "AbsPercErr_naive", "AbsPercErr_lm","AbsPercErr_gam")]
MAE_forplot <- gather(MAE_forplot, "Model", "PercentError", AbsPercErr:AbsPercErr_gam)
MAE_forplot <- data.table(MAE_forplot)[, mean(PercentError), by = list(state_name, Model)]
colnames(MAE_forplot) <- c("state_name", "Model", "MAPE")
MAE_forplot$Model <- as.factor(MAE_forplot$Model)
levels(MAE_forplot$Model) <- c( "XGBoost", "GAM", "Linear Model", "Naive")

BiggestStates <- data.table(state_df)[, sum(Value), by = state_name]
BiggestStates <- BiggestStates[with(BiggestStates, order(-V1)),]

ggplot(MAE_forplot[MAE_forplot$state_name %in% BiggestStates$state_name[1:10],],
       aes(x = reorder(state_name, MAPE), y = MAPE, fill = Model))+
  geom_col(width = 0.7, alpha = 0.7, position = "dodge") +
  coord_flip() + theme_bw() + ylab("Mean Avg Perc Error (%)") + xlab("State") +
  theme(legend.position = "bottom")

```

Although the more complex models outperform the linear model for some states, overall there is a negligible difference in performance between the three non-naive models when averaged across all states.

```{r}
table_forDisplay <- data.frame("Model" = c("Naive", "Linear Model", "XGBoost", "GAM"),
                               "MAE (acres/state)" = c(signif(DescTools::MAE(totalresult_xgb_DF$Value, totalresult_xgb_DF$naive, na.rm = TRUE), digits = 3),
                                      signif(DescTools::MAE(totalresult_xgb_DF$Value, totalresult_xgb_DF$lm, na.rm = TRUE), digits = 3),
                                      signif(DescTools::MAE(totalresult_xgb_DF$Value, totalresult_xgb_DF$.pred, na.rm = TRUE), digits = 3),
signif(DescTools::MAE(totalresult_xgb_DF$Value, totalresult_xgb_DF$gam, na.rm = TRUE), digits = 3)))

knitr::kable(table_forDisplay, align = c(rep('c', 2)), caption = "Error Metrics Associated with Different Models", booktabs = T,
                     col.names = c("Model", "MAE (acres/state)"))

```

In order to confirm our subjective judgment that "there is no difference" between the simple linear model and the two more complex models, we ran a 5000 iteration bootstrap to generate a distribution and estimate a 95% confidence interval. Below we see that although neither the XGBoost nor GAM models fall outside the bounds of the linear models expected error range, the linear model definitively produces an error less than that of the naive prediction.



```{r}
if(file.exists("../.././data/processed/BootstrappedMAE.csv")){
  MAEs <- read.csv("../.././data/processed/BootstrappedMAE.csv")
  nboots <- 5000
}else{
  source("../.././randomforest/BootstrappingErrorMetrics_StateLevel.R")
}

ggplot(MAEs, aes(x = MAE)) + geom_histogram(color="#00314E", fill="#01BEFF") + theme_bw() + xlab("MAE Distribution (Hundred Thousand Acres)") +
  ylab("") + theme(axis.title.y=element_blank(),
                   axis.text.y=element_blank(),
                   axis.ticks.y=element_blank()) +
    geom_vline(xintercept = MAEs[0.95*nboots, "MAE"], color = "#00314E", linetype = "dashed") +
    geom_vline(xintercept = MAEs[0.05*nboots, "MAE"], color = "#00314E", linetype = "dashed") +
   annotate("text", x = MAEs[0.95*nboots, "MAE"] + 0.7, y = 500, label = paste0("95% CI: (", round(MAEs[0.05*nboots, "MAE"], 2), ", ",
                                                                          round(MAEs[0.95*nboots, "MAE"], 2), ")"),
            colour = "#00314E")
```

Recall that for the national case, the team could produce estimates for national corn acreage planting with an MAE of roughly 1.2 million acres. If we aggregate our state level predictions up to the national level, we observe an MAE of 3.8 - 4.6 million acres. Note that the more complex models (XGBoost and GAM) do perform substantially better than the linear model at this level of aggregation. This suggests that the linear model performs better in states that grow less corn, while the XGBoost and GAM perform better in states that grow more corn. While we previously asserted that we recommend pursuing the simple linear model because its simplicity belied the negligible improvements by the GAM and XGBoost model, this particular observation indicates that it may be worth a discussion of the relative importance of estimating planting acres in a state like Illinois as compared to Connecticut, for example.



```{r, display = FALSE}
nationalResults <- data.table(totalresult_xgb_DF)[, .(sum(Value), sum(lm), sum(gam), sum(.pred)), by = year]

colnames(nationalResults) <- c("year", "Truth", "LinearModelPrediction", "GAMPrediction", "XGBoostPrediction")

national_forDisplay <- data.frame("Model" = c("Previously Described National Model", "Linear Model", "GAM", "XGBoost"),
                                  "MAE" = c("1200000",
                                  signif(DescTools::MAE(nationalResults$Truth, nationalResults$LinearModelPrediction, na.rm = TRUE), digits = 3),
                                  signif(DescTools::MAE(nationalResults$Truth, nationalResults$GAMPrediction, na.rm = TRUE), digits = 3),
            signif(DescTools::MAE(nationalResults$Truth, nationalResults$XGBoostPrediction, na.rm = TRUE), digits = 3)
                      
))

knitr::kable(national_forDisplay, align = c(rep('c', 2)), caption = "Error Metrics Associated with Nationally Aggregated Models", booktabs = T,
                     col.names = c("Model", "MAE (acres)"))


```

It is not surprising that aggregated state level projections are not as good as a single national projection. As a problem increases in granularity, its difficulty increases. Further, we had relevant data available at the national level (USDA published projections) that does not exist at the state level (except via rough extrapolation). The national model previously developed by the BPI team is still a relevant and important tool, provided the business wants to be able to estimate national planting acreage for corn.

## Conclusions

The BCS team can produce a model that is 40 - 80% better than the naive case. On average, the BCS team can predict the state-wide planting acreage to within 140,000 acres (note that the average state evaluated in this document plants 1.8 million acres/year). The state-level model does not substantially improve with the addition of in-season sales data. This means that the BCS team can produce their prediction for the upcoming market year as soon as the acreage planting for the prior season becomes available. Given our teams observations of the USDA's state acreage publication rate, it seems likely that the team could produce a model as early as June 2021 for a 2022 planting season. 

Due to the large discrepancy between the national level MAE and the state level MAE, we suggest that if the business is interested in both state and national level projections, two independent models should be implemented.
